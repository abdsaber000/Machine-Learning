# -*- coding: utf-8 -*-
"""gredient descent with linear regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hftcj5BDKlVqMATOQswMIvUCMhKRzaGF
"""

import numpy as np
from matplotlib import pyplot as plt

x_points = [1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
y_points = [1, 2, 3, 1, 4, 5, 6, 4, 7 , 10, 15, 9]

plt.plot(x_points , y_points , 'bo')

# line equation : y = m * x + b

# set both to initial value

m = 0
b = 0

y = lambda x : m * x + b

def plot_line (y , data):
  min_x = int(min(data)) - 1;
  max_x = int(max(data)) + 2;

  x_points = [i for i in range(min_x , max_x)]

  y_points = [y(x) for x in x_points]

  plt.plot(x_points , y_points)

plot_line(y , x_points);

"""Gredient Descent equations:

$m = m - \alpha \frac{1}{k} \sum_{i=1}^k(f(x^i)-y^i) x^i$

$b = b - \alpha \frac{1}{k} \sum_{i=1}^k(f(x^i)-y^i)$
"""

learn_rate = 0.001 #alpha

def summation (y, x_points,  y_points):
  total_m = 0
  total_b = 0
  k = len(x_points)
  for i in range(k):
    total_m += (y(x_points[i]) - y_points[i]) * x_points[i]
    total_b += (y(x_points[i]) - y_points[i])

  return total_m  / k , total_b / k

# Do 10 iterations

for i in range(10000):
  sum1 , sum2 = summation(y , x_points , y_points)
  m = m - learn_rate * sum1
  b = b - learn_rate * sum2

b

m

plot_line(y, x_points)
plt.plot(x_points, y_points, 'bo')